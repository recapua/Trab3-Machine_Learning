{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar\n",
    "import pandas\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "from numpy.linalg import norm\n",
    "from matplotlib.colors import ListedColormap\n",
    "import random\n",
    "import time\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe com 581012 exemplares e 12 atributos importado com sucesso\n"
     ]
    }
   ],
   "source": [
    "#importação da base de dados\n",
    "#os dados estão disponíveis abertamente em http://archive.ics.uci.edu/ml/datasets/Covertype\n",
    "\n",
    "file = \"covtype.data\"\n",
    "\n",
    "#nome dos atributos\n",
    "#estamos descartando os atributos que descrevem o tipo do solo\n",
    "names = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area_Rawah', 'Wilderness_Area_Neota', 'Wilderness_Area_Comanche', 'Wilderness_Area_Cache', 'Cover_Type']\n",
    "\n",
    "#de 0 a 13 são os atributos listados acima, e 54 é a classificação\n",
    "usecols = list(range(0, 14)) + [54]\n",
    "\n",
    "#especifico o tipo de alguns parametros(os que não são simplesmente numéricos)\n",
    "dtype = {'Cover_Type': 'category', 'Wilderness_Area_Rawah' : bool, 'Wilderness_Area_Neota' : bool, 'Wilderness_Area_Comanche' : bool, 'Wilderness_Area_Cache' : bool}\n",
    "\n",
    "#lê o arquivo num pandas.dataframe\n",
    "dataset = pandas.read_csv(file, header = None, usecols = usecols, names = names, dtype = dtype)\n",
    "\n",
    "#adicionando uma coluna adicional para sintetizar os 4 boleanos que representam a Wilderness_area. \n",
    "#para uma única instância, somente um dos 4 booleanos pode ser verdadeiro, logo eles, em realidade, funcionam como uma categorização\n",
    "new_column = pandas.Series([1 if dataset['Wilderness_Area_Rawah'][i] else \n",
    "                            2 if dataset['Wilderness_Area_Neota'][i] else\n",
    "                            3 if dataset['Wilderness_Area_Comanche'][i] else\n",
    "                            4 for i in range(len(dataset.index)) ], dtype=\"category\")\n",
    "#elimina as colunas reduzidas\n",
    "dataset = dataset.drop(columns=['Wilderness_Area_Rawah', 'Wilderness_Area_Neota', 'Wilderness_Area_Comanche', 'Wilderness_Area_Cache'])\n",
    "#insere nova coluna na posição 10\n",
    "dataset.insert(loc = 10, column = 'Wilderness_Area', value = new_column)\n",
    "\n",
    "#atualiza names para refletir a mudança acima\n",
    "names = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area', 'Cover_Type']\n",
    "\n",
    "print(\"Dataframe com %d exemplares e %d atributos importado com sucesso\" % (dataset.values.shape[0], dataset.values.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test options and evaluation metric\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "#os atributos para treino em X e a classificação correta em Y\n",
    "X = dataset.values[:, 0:11]\n",
    "Y = dataset.values[:, 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(581012, 11) (464809, 11) (116203, 11)\n",
      "(581012, 11) (464809, 11) (116203, 11) (581012, 11)\n",
      "(581012,) (464809,) (116203,) (581012,)\n"
     ]
    }
   ],
   "source": [
    "array = dataset.values\n",
    "X = array[:, 0:11]\n",
    "Y = array[:, 11]\n",
    "\n",
    "\n",
    "#preprocessa dataset\n",
    "#estava preprocessando, mas não fazia diferença\n",
    "#X_scale_temp = preprocessing.scale(X[:, 0:10])\n",
    "#X_scaled = np.append(X_scale_temp, X[:, 10].reshape(-1, 1), axis = 1)\n",
    "\n",
    "#separamos um pedaço do dataset para usarmos de varificação depois\n",
    "validation_size = 0.2\n",
    "seed = 7\n",
    "#estratificado por Y\n",
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size = validation_size, stratify = Y)#, random_state = seed**2)\n",
    "\n",
    "print(X.shape, X_train.shape, X_validation.shape)\n",
    "X_all = np.append(X_train, X_validation, axis = 0)\n",
    "Y_all = np.append(Y_train, Y_validation, axis = 0)\n",
    "\n",
    "print(X.shape, X_train.shape, X_validation.shape, X_all.shape)\n",
    "print(Y.shape, Y_train.shape, Y_validation.shape, Y_all.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usamos um 10-fold para estimar a acurácia do método\n",
    "def EstimarAcuraciaMetodo(metodo, k = 10):\n",
    "    kfold = model_selection.KFold(n_splits = k, shuffle = True, random_state = 121)#, random_state = seed)\n",
    "    cv_results = model_selection.cross_validate(metodo, X, Y, cv = kfold, scoring = scoring)\n",
    "    print(cv_results)\n",
    "    msg = \"%s média entre os %d folds: %f, com desvio padrão de %f ,em \" % (scoring, k, cv_results['test_score'].mean(), cv_results['test_score'].std())\n",
    "    print(msg)\n",
    "    msg2 = \"tempo médio de treino: %f, para %d samples por fit\" % (cv_results['fit_time'].mean(), (len(X_train) // k) * (k-1))\n",
    "    print(msg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([7.24382329, 7.16426277, 7.01238894, 7.14673424, 7.39530301,\n",
      "       7.16332293, 8.10411382, 8.58672857, 7.34939957, 6.99550843]), 'score_time': array([0.13495636, 0.11695266, 0.10936713, 0.12439513, 0.10895491,\n",
      "       0.09376311, 0.20850873, 0.15769482, 0.10936761, 0.09974313]), 'test_score': array([0.93139651, 0.93120719, 0.93034543, 0.93239359, 0.93161908,\n",
      "       0.92974303, 0.92831449, 0.93118879, 0.93084456, 0.93001842]), 'train_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}\n",
      "accuracy média entre os 10 folds: 0.930707, com desvio padrão de 0.001091 ,em \n",
      "tempo médio de treino: 7.416159, para 418320 samples por fit\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "EstimarAcuraciaMetodo(dtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time in seconds, for 464809 samples: 6.859375\n",
      "0.9276008364672168\n",
      "[[39317  2773     4     0    42     6   226]\n",
      " [ 2675 53270   212     4   308   152    40]\n",
      " [    5   237  6475    70    21   343     0]\n",
      " [    0     1    68   450     0    30     0]\n",
      " [   41   281    25     0  1538    13     1]\n",
      " [   12   171   316    35     7  2932     0]\n",
      " [  260    34     0     0     0     0  3808]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.93      0.93      0.93     42368\n",
      "          2       0.94      0.94      0.94     56661\n",
      "          3       0.91      0.91      0.91      7151\n",
      "          4       0.81      0.82      0.81       549\n",
      "          5       0.80      0.81      0.81      1899\n",
      "          6       0.84      0.84      0.84      3473\n",
      "          7       0.93      0.93      0.93      4102\n",
      "\n",
      "avg / total       0.93      0.93      0.93    116203\n",
      "\n",
      "Tempo médio para uma predição: 0.000033 segundos\n"
     ]
    }
   ],
   "source": [
    "#verificamos a acurácia do método aplicando-o sobre o set de validação que separamos anteriormente\n",
    "dtc3 = DecisionTreeClassifier()\n",
    "t0 = time.process_time()\n",
    "dtc3.fit(X_train, Y_train)\n",
    "tf = time.process_time()\n",
    "print(\"Training time in seconds, for %d samples: %f\" % (len(X_train), tf - t0))\n",
    "predictions = dtc3.predict(X_validation)\n",
    "print(accuracy_score(Y_validation, predictions))\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "print(classification_report(Y_validation, predictions))\n",
    "reshape = X_validation[0].reshape(1, -1)\n",
    "N = 1000\n",
    "print(\"Tempo médio para uma predição: %f segundos\" % (timeit.timeit(\"dtc3.predict(reshape)\", \"from __main__ import dtc3, reshape\", number = N) / N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 11)\n",
    "AvaliarMetodo(dtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time in seconds, for 464809 samples: 2.359375\n",
      "0.95926955414232\n",
      "[[40503  1720     4     0    28     2   111]\n",
      " [ 1346 54985   111     1   125    74    19]\n",
      " [    2   121  6868    30     1   129     0]\n",
      " [    0     4    92   414     0    39     0]\n",
      " [   21   209    23     0  1640     6     0]\n",
      " [    2   122   211    11     5  3122     0]\n",
      " [  147    16     0     0     1     0  3938]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.96      0.96      0.96     42368\n",
      "          2       0.96      0.97      0.97     56661\n",
      "          3       0.94      0.96      0.95      7151\n",
      "          4       0.91      0.75      0.82       549\n",
      "          5       0.91      0.86      0.89      1899\n",
      "          6       0.93      0.90      0.91      3473\n",
      "          7       0.97      0.96      0.96      4102\n",
      "\n",
      "avg / total       0.96      0.96      0.96    116203\n",
      "\n",
      "Tempo médio para uma predição: 0.000431 segundos\n"
     ]
    }
   ],
   "source": [
    "#verificamos a acurácia do método aplicando-o sobre o set de validação que separamos anteriormente\n",
    "knn = KNeighborsClassifier(n_neighbors = 11)\n",
    "t0 = time.process_time()\n",
    "knn.fit(X_train, Y_train)\n",
    "tf = time.process_time()\n",
    "print(\"Training time in seconds, for %d samples: %f\" % (len(X_train), tf - t0))\n",
    "predictions = knn.predict(X_validation)\n",
    "print(accuracy_score(Y_validation, predictions))\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "print(classification_report(Y_validation, predictions))\n",
    "reshape = X_validation[0].reshape(1, -1)\n",
    "N = 1000\n",
    "print(\"Tempo médio para uma predição: %f segundos\" % (timeit.timeit(\"knn.predict(reshape)\", \"from __main__ import knn, reshape\", number = N) / N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN with three attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time in seconds, for 464809 samples: 1.343750\n",
      "0.818662168790823\n",
      "[[34468  7300    29     0    47    15   509]\n",
      " [ 6569 48671   677     0   416   290    38]\n",
      " [   33   874  5495   128    17   604     0]\n",
      " [    0     3   227   256     0    63     0]\n",
      " [  112   771    40     0   970     6     0]\n",
      " [   19   450   883    38    10  2073     0]\n",
      " [  806    98     0     0     0     0  3198]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.82      0.81      0.82     42368\n",
      "          2       0.84      0.86      0.85     56661\n",
      "          3       0.75      0.77      0.76      7151\n",
      "          4       0.61      0.47      0.53       549\n",
      "          5       0.66      0.51      0.58      1899\n",
      "          6       0.68      0.60      0.64      3473\n",
      "          7       0.85      0.78      0.82      4102\n",
      "\n",
      "avg / total       0.82      0.82      0.82    116203\n",
      "\n",
      "Tempo médio para uma predição: 0.000349 segundos\n"
     ]
    }
   ],
   "source": [
    "#verificamos a acurácia do método aplicando-o sobre o set de validação que separamos anteriormente\n",
    "\n",
    "#seleciono os tres parametros mais relevantes, baseado na minha exploração dos dados\n",
    "X_3 = np.zeros((dataset.values.shape[0], 3))\n",
    "X_3[:, 0] = dataset['Elevation']\n",
    "X_3[:, 1] = dataset['Horizontal_Distance_To_Roadways']\n",
    "X_3[:, 2] = dataset['Horizontal_Distance_To_Fire_Points']\n",
    "\n",
    "#separamos um pedaço do dataset para usarmos de varificação depois\n",
    "validation_size = 0.2\n",
    "seed = 7\n",
    "#estratificado por Y\n",
    "X_train_3, X_validation_3, Y_train_3, Y_validation_3 = model_selection.train_test_split(X_3, Y, test_size = validation_size, stratify = Y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "knn3 = KNeighborsClassifier(n_neighbors = 5)\n",
    "t0 = time.process_time()\n",
    "knn3.fit(X_train_3, Y_train_3)\n",
    "tf = time.process_time()\n",
    "print(\"Training time in seconds, for %d samples: %f\" % (len(X_train_3), tf - t0))\n",
    "predictions = knn3.predict(X_validation_3)\n",
    "print(accuracy_score(Y_validation_3, predictions))\n",
    "print(confusion_matrix(Y_validation_3, predictions))\n",
    "print(classification_report(Y_validation_3, predictions))\n",
    "reshape = X_validation_3[0].reshape(1, -1)\n",
    "N = 1000\n",
    "print(\"Tempo médio para uma predição: %f segundos\" % (timeit.timeit(\"knn3.predict(reshape)\", \"from __main__ import knn3, reshape\", number = N) / N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
