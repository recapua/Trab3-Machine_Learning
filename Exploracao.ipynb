{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Define Problem.\n",
    "    Prepare Data.\n",
    "    Evaluate Algorithms.\n",
    "    Improve Results.\n",
    "    Present Results.\n",
    "    \n",
    "    http://archive.ics.uci.edu/ml/datasets/Covertype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "from numpy.linalg import norm\n",
    "from matplotlib.colors import ListedColormap\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"covtype.data\"\n",
    "#nome dos atributos\n",
    "#estamos descartando os atributos que descrevem o tipo do solo\n",
    "names = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area_Rawah', 'Wilderness_Area_Neota', 'Wilderness_Area_Comanche', 'Wilderness_Area_Cache', 'Cover_Type']\n",
    "#names = ['Elevation', 'Aspect', 'Slope', 'Cover_Type']\n",
    "\n",
    "#usecols = [0..13, 54]\n",
    "\n",
    "usecols = list(range(0, 14)) + [54]\n",
    "#usecols = list(range(0, 3)) + [54]\n",
    "\n",
    "#especifico o tipo de alguns parametros(os que não são simplesmente numéricos)\n",
    "dtype = {'Cover_Type': 'category', 'Wilderness_Area_Rawah' : bool, 'Wilderness_Area_Neota' : bool, 'Wilderness_Area_Comanche' : bool, 'Wilderness_Area_Cache' : bool}\n",
    "\n",
    "dataset = pandas.read_csv(file, header = None, usecols = usecols, names = names, dtype = dtype)\n",
    "\n",
    "#adicionando uma coluna adicional para sintetizar os 4 boleanos que representam a Wilderness_area. \n",
    "#para uma única instância, somente um dos 4 booleanos pode ser verdadeiro, logo eles, em realidade, funcionam como uma categorização\n",
    "#\n",
    "new_column = pandas.Series([1 if dataset['Wilderness_Area_Rawah'][i] else \n",
    "                            2 if dataset['Wilderness_Area_Neota'][i] else\n",
    "                            3 if dataset['Wilderness_Area_Comanche'][i] else\n",
    "                            4 for i in range(len(dataset.index)) ], dtype=\"category\")\n",
    "#elimina as colunas reduzidas\n",
    "dataset = dataset.drop(columns=['Wilderness_Area_Rawah', 'Wilderness_Area_Neota', 'Wilderness_Area_Comanche', 'Wilderness_Area_Cache'])\n",
    "#insere nova coluna na posição 10\n",
    "dataset.insert(loc = 10, column = 'Wilderness_Area', value = new_column)\n",
    "\n",
    "names = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area', 'Cover_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.shape)\n",
    "#print(dataset.head(5))\n",
    "dataset.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantidade de exemplares por classificação\n",
    "dataset.groupby('Cover_Type').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,40)\n",
    "\n",
    "dataset.plot(kind='box', subplots=True, layout=(4,4), sharex=False, sharey=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = dataset.groupby('Cover_Type')\n",
    "for name in names:\n",
    "    print(name)\n",
    "    display(gp[name].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Elevation'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(dataset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#faz um scatter plot maneiro, seleto\n",
    "#para cada classificação, seleciona N amostras aleatórias pra serem plotadas\n",
    "seed = 7\n",
    "N = 250\n",
    "#markerTypes = {'1':'.', '2': 'x', '3': 'o', '4': '^', '5': '1', '6': '2', '7': '3'}\n",
    "markerTypes = {'1':'.', '2': '.', '3': '.', '4': '.', '5': '.', '6': '.', '7': '.'}\n",
    "points = []\n",
    "markers = []\n",
    "i = 0\n",
    "gp = dataset.groupby('Cover_Type', sort = False)\n",
    "samples = gp.apply(lambda x: x.sample(n = N, random_state = seed))\n",
    "\n",
    "#plota grafico name1 x name2\n",
    "name1, name2 = 'Elevation', 'Slope'\n",
    "i = 0\n",
    "fig, ax = plt.subplots()\n",
    "for cover_type, group in samples.groupby('Cover_Type'):\n",
    "    \n",
    "    plt.scatter(group[name1], group[name2], marker = markerTypes[cover_type])\n",
    "    \n",
    "ax.set(xlabel=name1, ylabel=name2,\n",
    "       title=name1 + ' x ' + name2)\n",
    "plt.show()\n",
    "\n",
    "#plota grafico name1 x name2\n",
    "name1, name2 = 'Elevation', 'Horizontal_Distance_To_Roadways'\n",
    "i = 0\n",
    "fig, ax = plt.subplots()\n",
    "for cover_type, group in samples.groupby('Cover_Type'):\n",
    "    \n",
    "    plt.scatter(group[name1], group[name2], marker = markerTypes[cover_type])\n",
    "    \n",
    "ax.set(xlabel=name1, ylabel=name2,\n",
    "       title=name1 + ' x ' + name2)\n",
    "plt.show()\n",
    "\n",
    "#plota grafico name1 x name2\n",
    "name1, name2 = 'Elevation', 'Horizontal_Distance_To_Fire_Points'\n",
    "i = 0\n",
    "fig, ax = plt.subplots()\n",
    "for cover_type, group in samples.groupby('Cover_Type'):\n",
    "    \n",
    "    plt.scatter(group[name1], group[name2], marker = markerTypes[cover_type])\n",
    "    \n",
    "ax.set(xlabel=name1, ylabel=name2,\n",
    "       title=name1 + ' x ' + name2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andre\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(581012, 1)\n"
     ]
    }
   ],
   "source": [
    "array = dataset.values\n",
    "X = array[:, 0:11]\n",
    "Y = array[:, 11]\n",
    "#preprocessa dataset\n",
    "X_scale_temp = preprocessing.scale(X[:, 0:10])\n",
    "print(X[:, 10].reshape(-1, 1).shape)\n",
    "X_scaled = np.append(X_scale_temp, X[:, 10].reshape(-1, 1), axis = 1)\n",
    "\n",
    "\n",
    "validation_size = 0.2\n",
    "seed = 7\n",
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size = validation_size, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_scaled[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test options and evaluation metric\n",
    "seed = 7\n",
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "there\n",
      "KNN: 0.631658 (0.026439)\n",
      "here\n",
      "there\n",
      "CART: 0.665721 (0.045751)\n"
     ]
    }
   ],
   "source": [
    "#testando varios algoritmos\n",
    "\n",
    "#custom metric:\n",
    "#calcula a distancia entre os vetores u e v. \n",
    "#É necessário customizar essa métrica porque o 'aspecto' não funciona da mesma maneira que as outras medidas: um aspecto de 359 grau é próximo ao de 0 grau, e não distante\n",
    "def custom_distance(u, v):\n",
    "    #para todos os campos \"normais\", só subtraio valores\n",
    "    custom = u - v\n",
    "    #mas, para o aspect, faço mod 360 . [1] é aspect\n",
    "    custom[1] = (u[1] - v[1]) % 360\n",
    "    dist = norm(u-v)\n",
    "    return dist\n",
    "\n",
    "\n",
    "\n",
    "models = []\n",
    "#models.append(('LR', LogisticRegression()))\n",
    "#models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "#usar uma função customizada aumentou estupidamente o tempo de processamento, sem gerar benefício algum\n",
    "#models.append(('KNN-Custom', KNeighborsClassifier(metric = custom_distance)))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "#models.append(('NB', GaussianNB()))\n",
    "#models.append(('SVM', SVC()))\n",
    "\n",
    "#avalia cada um deles\n",
    "results = []\n",
    "model_names = []\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits = 10, random_state = seed)\n",
    "    print(\"here\")\n",
    "    cv_results = model_selection.cross_val_score(model, X, Y, cv = kfold, scoring = scoring)\n",
    "    print(\"there\")\n",
    "    results.append(cv_results)\n",
    "    model_names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tentar descobrir o nivel de importancia dos atributos baseado na arvore de decisão\n",
    "dtc = DecisionTreeClassifier(random_state = seed)\n",
    "dtc.fit(X_train, Y_train)\n",
    "dtc.score(X_validation, Y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = dtc.feature_importances_\n",
    "for i in range(len(fi)):\n",
    "    msg = \"%s: %f\" % (names[i], fi[i])\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\n",
    "\n",
    "estimator = dtc\n",
    "\n",
    "n_nodes = estimator.tree_.node_count\n",
    "children_left = estimator.tree_.children_left\n",
    "children_right = estimator.tree_.children_right\n",
    "feature = estimator.tree_.feature\n",
    "threshold = estimator.tree_.threshold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The tree structure can be traversed to compute various properties such\n",
    "# as the depth of each node and whether or not it is a leaf.\n",
    "node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "while len(stack) > 0:\n",
    "    node_id, parent_depth = stack.pop()\n",
    "    node_depth[node_id] = parent_depth + 1\n",
    "\n",
    "    # If we have a test node\n",
    "    if (children_left[node_id] != children_right[node_id]):\n",
    "        stack.append((children_left[node_id], parent_depth + 1))\n",
    "        stack.append((children_right[node_id], parent_depth + 1))\n",
    "    else:\n",
    "        is_leaves[node_id] = True\n",
    "\n",
    "print(\"The binary tree structure has %s nodes and has \"\n",
    "      \"the following tree structure:\"\n",
    "      % n_nodes)\n",
    "for i in range(n_nodes):\n",
    "    if is_leaves[i]:\n",
    "        print(\"%snode=%s leaf node.\" % (node_depth[i] * \"\\t\", i))\n",
    "    else:\n",
    "        print(\"%snode=%s test node: go to node %s if X[:, %s] <= %s else to \"\n",
    "              \"node %s.\"\n",
    "              % (node_depth[i] * \"\\t\",\n",
    "                 i,\n",
    "                 children_left[i],\n",
    "                 feature[i],\n",
    "                 threshold[i],\n",
    "                 children_right[i],\n",
    "                 ))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Node count:\", n_nodes)\n",
    "print(\"Leaf node count:\", sum(b for b in is_leaves))\n",
    "print(\"Max node depth:\", node_depth.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's retrieve the decision path of each sample. The decision_path\n",
    "# method allows to retrieve the node indicator functions. A non zero element of\n",
    "# indicator matrix at the position (i, j) indicates that the sample i goes\n",
    "# through the node j.\n",
    "\n",
    "X_test = X_validation\n",
    "\n",
    "node_indicator = estimator.decision_path(X_test)\n",
    "\n",
    "# Similarly, we can also have the leaves ids reached by each sample.\n",
    "\n",
    "leave_id = estimator.apply(X_test)\n",
    "\n",
    "# Now, it's possible to get the tests that were used to predict a sample or\n",
    "# a group of samples. First, let's make it for the sample.\n",
    "\n",
    "sample_id = 0\n",
    "node_index = node_indicator.indices[node_indicator.indptr[sample_id]:\n",
    "                                    node_indicator.indptr[sample_id + 1]]\n",
    "\n",
    "print('Rules used to predict sample %s: ' % sample_id)\n",
    "for node_id in node_index:\n",
    "    if leave_id[sample_id] == node_id:\n",
    "        print(\"leaf node:\",  node_id, \"classification\", estimator.predict(X_test[sample_id].reshape(1, -1)))\n",
    "        continue\n",
    "\n",
    "    if (X_test[sample_id, feature[node_id]] <= threshold[node_id]):\n",
    "        threshold_sign = \"<=\"\n",
    "    else:\n",
    "        threshold_sign = \">\"\n",
    "\n",
    "    print(\"decision id node %s : (X_test[%s, %s] (= %s) %s %s)\"\n",
    "          % (node_id,\n",
    "             sample_id,\n",
    "             feature[node_id],\n",
    "             X_test[sample_id, feature[node_id]],\n",
    "             threshold_sign,\n",
    "             threshold[node_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
